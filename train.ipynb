{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fI4IGw5ckaTE"
   },
   "outputs": [],
   "source": [
    "is_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0mA7OFE1lODt"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import urllib\n",
    "\n",
    "#from google.colab import drive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LSTM\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal\n",
    "from torch_geometric_temporal.nn.recurrent import GConvLSTM\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "\n",
    "from evaluation import Evaluation\n",
    "from gconvlstm_lstm_model import LSTMSequenceModel\n",
    "from data_loder import DatasetLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X6Sg8NwXALR"
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rRyIFN-hIVz"
   },
   "source": [
    "## Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OT6G7ZA_EksZ"
   },
   "outputs": [],
   "source": [
    "def new_stations(): \n",
    "  ## Write the new stations! The unnecessary stations dont need delete.\n",
    "\n",
    "    # Data to be written\n",
    "    node_id = {\n",
    "      \"2275\": \"Szeged\",\n",
    "      \"1516\": \"Vásárosnemény\",\n",
    "      \"1719\": \"Tokaj\",\n",
    "      \"2543\": \"Tiszadorogma\",\n",
    "      \"2272\": \"Mindszent\",\n",
    "      \"2274\": \"Algyő\",\n",
    "      \"2756\": \"Gyoma\",\n",
    "      \"2278\": \"Makó\",\n",
    "      \"1722\": \"Tiszapalkonya\",\n",
    "      \"2753\": \"Békés\",\n",
    "      \"2271\": \"Csongrád\",\n",
    "      \"210888\": \"Zenta\"\n",
    "    }\n",
    "\n",
    "    # Serializing json\n",
    "    json_object = json.dumps(node_id, indent=12)\n",
    "\n",
    "    # Writing to parameters.json\n",
    "    with open(\"node_id.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n6DhuTdbjT6b"
   },
   "outputs": [],
   "source": [
    "def load_node_id(file_name:str):\n",
    "    with open(file_name, 'r') as openfile:\n",
    "            # Reading from json file\n",
    "        node_id = json.load(openfile)\n",
    "\n",
    "    return node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8YeIBK_3Kp2j"
   },
   "outputs": [],
   "source": [
    "def load_data(file_name:str,\n",
    "              data_slice: tuple = (1, 501)):\n",
    "    arr = np.loadtxt(file_name,  \n",
    "                  usecols = (np.r_[1:13]), \n",
    "                  # np.r_ generates an array of indices\n",
    "                  delimiter=\",\")\n",
    "\n",
    "    place_code = arr[0,:].astype(int)\n",
    "\n",
    "    start, end = data_slice\n",
    "    value = arr[start:end,:] \n",
    "\n",
    "    date = np.loadtxt(file_name,\n",
    "                  dtype='str',\n",
    "                  usecols = 0,\n",
    "                  skiprows=1,\n",
    "                  delimiter=\",\")\n",
    "    \n",
    "    actual_dates = date[start-1:end-1]\n",
    "\n",
    "    return value, place_code, actual_dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WEtG60gPegSG"
   },
   "outputs": [],
   "source": [
    "def preprocessing(values, train_ratio: float):\n",
    "    train_length= int(train_ratio * len(values))  \n",
    "\n",
    "    train = values[0:train_length]\n",
    "\n",
    "    std = np.std(train, axis=0)\n",
    "    mean = np.mean(train, axis=0)\n",
    "\n",
    "    return std, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JYnGav_gkJbo"
   },
   "outputs": [],
   "source": [
    "def get_params(value: np.array, \n",
    "              train_rat: float, \n",
    "              place_code: np.array, \n",
    "              id: dict, \n",
    "              save_name: str):\n",
    "\n",
    "    std, mean = preprocessing(values=value, train_ratio= train_rat)\n",
    "    d = dict(enumerate(place_code.flatten()))\n",
    "\n",
    "    data_dict={}\n",
    "    key_list = list(id.keys())\n",
    "    val_list = list(id.values())\n",
    "\n",
    "    for key, val in d.items():\n",
    "        position = key_list.index(str(val))\n",
    "        data_dict[val_list[position]]={\"column_idx\": key, \"std\": std[int(key)], \"mean\": mean[int(key)]}\n",
    "\n",
    "\n",
    "    json_object = json.dumps(data_dict, indent=12)\n",
    "\n",
    "    # Writing to parameters.json\n",
    "    with open(save_name + \".json\", \"w\") as outfile:   \n",
    "        outfile.write(json_object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0P7U2YdZhNt7"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4LCD2QnYo9Jy"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "if is_training:\n",
    "\n",
    "    data_n = \"data.csv\"     # the data file name\n",
    "    params_n = 'data_dict5'  # new name the parameters file\n",
    "    model_n = 'Model5'       # name the new model\n",
    "    past = 5              # how many days from the past are used \n",
    "    hidden_s = 50\n",
    "    distance_k = 2\n",
    "    \n",
    "    target_l = 7\n",
    "    split_ratio =  0.883       # ratio of train to data\n",
    "    d_slice = (1, 22001)       # start >= 1 !!!\n",
    "\n",
    "    value, place_code, act_dates = load_data(file_name=data_n, \n",
    "                                      data_slice=d_slice)\n",
    "\n",
    "    node_id = load_node_id(\"node_id.json\")\n",
    "\n",
    "    get_params(value=value, \n",
    "            train_rat=split_ratio, \n",
    "            place_code=place_code, \n",
    "            id=node_id, \n",
    "            save_name=params_n)\n",
    "\n",
    "    loader = DatasetLoader(data=data_n,                # data.csv read from workspace folder\n",
    "                         data_params=params_n+'.json',\n",
    "                         data_slice=d_slice) \n",
    "\n",
    "    dataset = loader.get_dataset(lags=past, target=target_l) # dataset is a StaticGraphTemporalSignal object\n",
    "\n",
    "    train_dataset, test_dataset = temporal_signal_split(data_iterator=dataset, \n",
    "                                                      train_ratio=split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_d3tEf8so7IR",
    "outputId": "d407c0ad-417f-4b3b-d0dc-7db0c1941a51",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [10:01<00:00,  3.01s/it]\n"
     ]
    }
   ],
   "source": [
    "if is_training:\n",
    "    try:\n",
    "      from tqdm import tqdm\n",
    "    except ImportError:\n",
    "      def tqdm(iterable):\n",
    "          return iterable\n",
    "\n",
    "\n",
    "    model = LSTMSequenceModel(dropout=0,\n",
    "                            hidden_size=hidden_s,\n",
    "                            k=distance_k,\n",
    "                            model_name=model_n,\n",
    "                            node_features=past,\n",
    "                            target_len=target_l,\n",
    "                            data_params=params_n+'.json')\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    all_mse = torch.tensor([])\n",
    "\n",
    "    for epoch in tqdm(range(200)): \n",
    "        cost = 0\n",
    "        h, c = None, None\n",
    "        for time, snapshot in enumerate(train_dataset):\n",
    "        # train_dataset is a StaticGraphTemporalSignal\n",
    "        # Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=y)\n",
    "        #          x: torch.Tensor with shape number of stations*node_feature  \n",
    "        # edge_index: torch.Tensor with shape MassigePassing   \n",
    "        #  edge_attr: weight of edges\n",
    "        #          y: the target lable for each station\n",
    "\n",
    "        ##  edge_index and edge_attr are same for every time, because the graph is Static\n",
    "            snapshot = snapshot.to(device)\n",
    "            y_hat, idx_Szeged = model(x=snapshot.x, \n",
    "                                    edge_index=snapshot.edge_index, \n",
    "                                    edge_weight=snapshot.edge_attr, \n",
    "                                    h=h, \n",
    "                                    c=c)\n",
    "            y_hat = y_hat.to(device)\n",
    "            cost = cost + (((y_hat.T-snapshot.y[idx_Szeged, :])**2).sum()/target_l)\n",
    "        \n",
    "        cost = cost / (time+1)\n",
    "\n",
    "        mse = torch.tensor([cost])\n",
    "        all_mse = torch.cat((all_mse, mse))\n",
    "\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval() \n",
    "\n",
    "    # Save to file    \n",
    "    torch.save(model.state_dict(), model_n)\n",
    "\n",
    "    # Save MSE\n",
    "    np.savetxt(fname=model_n + \"_MSE.csv\", delimiter=\",\", X=all_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "mnNLpHXKnMqK",
    "aIxF532jAjbo",
    "q_EVxTkmDldz",
    "7X6Sg8NwXALR",
    "4rRyIFN-hIVz",
    "0P7U2YdZhNt7"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
